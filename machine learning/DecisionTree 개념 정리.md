# :mag: Index

- [의사결정나무(Decision Tree)는 뭘까?](#idx1) 
- [의사결정 나무의 철학: Information Gain](#idx2) 
- [why?](#day)
- ㅇ
- ㅇ

---

### :radio_button: 의사결정나무(Decision Tree)는 뭘까?  <a id="idx1"></a>


의사결정나무란 데이터 사이 존재하는 __패턴__ 을 찾아 이 규칙들의 __조합__ 으로 예측 모델을 만드는데 쓰이는 알고리즘으로 기계학습의 기초가 되는 알고리즘 중 하나이다.



그림으로 그렸을때 트리의 형태를 이루기 때문에 '나무' 라는 말이 붙었는데 이는 추후 여러 의사결정나무를 앙상블 기법을 통해 활용한 __Random Forest__ 알고리즘에도 영향을 주었다.



의사결정나무는 설명변수를 __\*하나씩만\*__ 활용하여 가지뻗기를 진행하는데 이는 우리가 어릴적 한번씩 해봤던 '스무고개' 놀이를 통해 이해할 수 있다. 

> 의사결정나무에 대한 설명을 보면 `설명변수`라는 용어가 쓰이는데 `입력특성`,`독립변수` 정도로 이해하면 된다.

![스무고개 그림](https://t1.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/1oU7/image/OG5rMwYpRcG4gRPFv8qgLslNLTQ.png)

[이미지 출처](https://www.google.com/url?sa=i&url=https%3A%2F%2Fbrunch.co.kr%2F%40kakao-it%2F157&psig=AOvVaw1D35xHZ-Geohyk-iCIprD-&ust=1601041536507000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCIDxttT2gewCFQAAAAAdAAAAABAX)



다음과 같은 스무고개 놀이를 했을 때 __`다리`__ ,__`고양이와 크기가 비슷`__ , __`바다에 사는`__ 은 모두 위에서 언급한 설명변수에 해당한다. 데이터에 대한 패턴을 찾아 트리 구조로 규칙을 조합해놓은 모양이다. 한번에 설명변수 하나씩만을 이용해 가지를 뻗게되어 __terminal node__ , __leaf node__ 라고 부르는 트리의 최하위 항목들을 살펴보면 초기 데이터의 상호 베타적인 집합으로 분류되어 있는것을 확인할 수 있다.



의사결정나무는 이런 모양으로 규칙을 조합하여 데이터에 대한 예측 모델을 이룬다. 이는 `분류(Classification)` 문제와 `회귀(Regression)` 문제 모두 적용가능한데 , 분류 문제의 경우 해당 데이터가 최종적으로 속한 최하위 그룹의 최빈값을 예측값으로 활용하고 회귀 문제의 경우 그 그룹내 데이터의 평균값을 예측값으로 활용한다.즉, N개의 최하위 노드를 갖는 의사결정나무로부터 얻을 수 있는 예측값의 종류는 `분류|회귀 문제와 상관없이 동일하게 최대 N개이다.`

---

### :radio_button: 의사결정 나무의 철학: Information Gain <a id="idx2"></a>

앞에서 언급한 스무고개 놀이를 하는데 만약 첫 질문에 __'혹시 매운 음식인가요?'__ 라고 물었는데 __'네'__ 라는 결과가 나온다면 우리는 __'나이스!'__ 를 외칠것이다. 왜일까?



이는 우리가 고려해야하는 정답 후보 데이터가 아주 세부적으로 바뀌었기 때문이다. 의사결정나무에서는 이를 __`정보획득(Information Gain)`__ 이라 부른다. 정확한 의미는 의사결정나무에서 분기가 일어날때 데이터의 순도 변화량을 정보획득이라 부른다. 데이터의 __순도(homogeneity)__ 란 말 그대로 데이터가 얼마나 단조로운/변화가 많은 혹은 종류가 적은/많은가 정도로 이해하면 되는데 이와 반대되는 개념으로 __불순도(impurity)__ 를 통해 정보획득을 분기 전후의 불순도 감소정도로 설명하기도 한다.



> 방금 정보획득을 순도의 증가량 , 불순도의 감소량 이라는 표현을 통해 설명하였는데 그러면 이는 수치적으로 어떻게 표현될까?



순도,불순도를 수치로 나타내기 위한 종류는 대표적으로 3가지가 있다. 바로 `Entropy` , `Gini Index` , `Misclassification Error` 인데, 이 중 `Entropy` 지표에 대해 알아보자면 다음과 같은 식으로 정의된다.

![초기 엔트로피](../assets/inital_entropy.PNG)

초기 m 개의 분류를 가진 데이터에 대해서는 위의 식 처럼 각 분류의 확률과 확률의 로그값을 모두 더한 값이 초기 엔트로피 값이다. 이 상태에서 데이터나 나뉘게 되면 다음과 같은 방식으로 엔트로피를 구하게 된다.

![일반 엔트로피](../assets/general_entropy.PNG)

각 나뉜 각 그룹에 대해 엔트로피를 구한 다음 그룹의 상대 부피인 R 값을 곱하여 다시 더하는 식이다. 

 

의사결정나무의 학습방법에서 핵심은 저 엔트로피 지수가 낮아지는 쪽으로 분기를 하는것이다.





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:day





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목





### :arrow_right:항목
