# Course Study from [CSC321](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/) 



### [2021.01.09]

__`Topic`__ : **“Neural Network quick review to Gradient vanishing & exploding problem”** 

__`Notes`__ : 

- [https://drive.google.com/file/d/1YzuEcHHXb7-nSEZ-zwTr-MvTCmbHEK-s/view?usp=sharing](https://drive.google.com/file/d/1YzuEcHHXb7-nSEZ-zwTr-MvTCmbHEK-s/view?usp=sharing) 

__`Links`__ :

- [https://wikidocs.net/61375](https://wikidocs.net/61375) 
- [https://m.blog.naver.com/PostView.nhn?blogId=pshkhh&logNo=221203426679&proxyReferer=https:%2F%2Fwww.google.com%2Fhttps://brunch.co.kr/@chris-song/39](https://m.blog.naver.com/PostView.nhn?blogId=pshkhh&logNo=221203426679&proxyReferer=https:%2F%2Fwww.google.com%2Fhttps://brunch.co.kr/@chris-song/39) 
- [https://t-lab.tistory.com/14](https://t-lab.tistory.com/14) 
- [https://aikorea.org/blog/rnn-tutorial-3/](https://aikorea.org/blog/rnn-tutorial-3/) 
- [https://velog.io/@dscwinterstudy/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D2-5%EC%9E%A5-gnk6bhirc3](https://velog.io/@dscwinterstudy/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D2-5%EC%9E%A5-gnk6bhirc3) 
- [https://m.blog.naver.com/worb1605/221187949828](https://m.blog.naver.com/worb1605/221187949828) 
- [https://ganghee-lee.tistory.com/30](https://ganghee-lee.tistory.com/30) 
- [https://heehehe-ds.tistory.com/entry/Deep-Learning-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98loss-function-%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80optimizer](https://heehehe-ds.tistory.com/entry/Deep-Learning-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98loss-function-%EC%98%B5%ED%8B%B0%EB%A7%88%EC%9D%B4%EC%A0%80optimizer) 
- [https://ydseo.tistory.com/41](https://ydseo.tistory.com/41) 
- [https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html](https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html) 
- [https://simonjisu.github.io/numpyseries/2018/03/14/rnnlstm2.html](https://simonjisu.github.io/numpyseries/2018/03/14/rnnlstm2.html) 
- [https://mmuratarat.github.io/2019-02-07/bptt-of-rnn](https://mmuratarat.github.io/2019-02-07/bptt-of-rnn) 
- [https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/) 

__`Next`__ : 2021.01.16 9:30 PM KST

- [http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15 Exploding and Vanishing Gradients.pdf)
- __Lecture 5: Multilayer Perceptrons (박희원) , Lecture 6: Backpropagation (이동재)__ 

---



### [2021.01.16]

__`Topic`__  : __"Lecture 5: Multilayer Perceptrons" (박희원) , "Lecture 6: Backpropagation" (이동재)__ 

__`Notes`__  : 

- [https://drive.google.com/file/d/1Vve7AgAP2jnNuPpmrtkD8uRsJr_RdlBv/view?usp=sharing](https://drive.google.com/file/d/1Vve7AgAP2jnNuPpmrtkD8uRsJr_RdlBv/view?usp=sharing) (희원)
- [https://drive.google.com/file/d/1V_E_m11_Ebj-GQP1qVvg0nt-Q6CdLhFY/view?usp=sharing](https://drive.google.com/file/d/1V_E_m11_Ebj-GQP1qVvg0nt-Q6CdLhFY/view?usp=sharing) (동재)

__`Links`__ : 

>  Multilayer Perceptrons 

- [http://hleecaster.com/ml-perceptron-concept/](http://hleecaster.com/ml-perceptron-concept/) 
- [http://blog.naver.com/PostView.nhn?blogId=apr407&logNo=221238611771&parentCategoryNo=&categoryNo=55&viewDate=&isShowPopularPosts=true&from=search](http://blog.naver.com/PostView.nhn?blogId=apr407&logNo=221238611771&parentCategoryNo=&categoryNo=55&viewDate=&isShowPopularPosts=true&from=search) 
- [https://untitledtblog.tistory.com/27](https://untitledtblog.tistory.com/27) 
- [https://choosunsick.github.io/post/neural_network_intro/](https://choosunsick.github.io/post/neural_network_intro/) 
- [http://www.datamarket.kr/xe/board_LCmL04/26245](http://www.datamarket.kr/xe/board_LCmL04/26245) 

> Backpropagation

- [https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L06%20Backpropagation.pdf](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L06%20Backpropagation.pdf) 
- [https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec8a.pdf](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec8a.pdf) 
- [https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec8b.pdf](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec8b.pdf) 

__`Next`__ : 2021.01.23 9:00 PM KST

- [http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/) 
- __Lecture 7: Optimization (이찬주)__ 

---



### [2021.01.23]

__`Topic`__ : __Lecture 7: Optimization 1st part (이찬주)__ 

__`Notes`__ :

- [https://drive.google.com/file/d/1i8Q51GrKBykL9MsNINdWmhOfUZLekTMW/view?usp=sharing](https://drive.google.com/file/d/1i8Q51GrKBykL9MsNINdWmhOfUZLekTMW/view?usp=sharing) 

__`Links`__ : 

- [https://hyunw.kim/blog/2017/11/01/Optimization.html](https://hyunw.kim/blog/2017/11/01/Optimization.html) 
- [Gradient Descent Optimization Algorithms 정리](http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html) , (shuuki4.github.io)

__`Next`__ : 2021.01.30 9:00 PM KST 

- [http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/) 
- __Lecture 7: Optimization (from Momentum 민채정)__ 

---



### [2021.01.30]

__`Topic`__  **:** **Lecture 7: Optimization (2nd , 민채정)**

__`Notes`__ : 업로드 예정

__`Links`__ : 

- [Derivative of Logistic Regression](https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d)
- [what is convex function?](https://ratsgo.github.io/convex%20optimization/2017/12/26/convexfunction/)
- [cost-function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)
- [https://www.slideshare.net/hyeseunglee6/ch6-75888743?from_action=save](https://www.slideshare.net/hyeseunglee6/ch6-75888743?from_action=save) 
- [https://sacko.tistory.com/42https://twinw.tistory.com/247](https://sacko.tistory.com/42https://twinw.tistory.com/247) 

__`Next`__ : 2021.02.06 9:00 PM KST

- [http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/) 
- __Lecture 9: Generalization 1st part ( ~weight decay , 이동재)__ 
- 순서 고정 : `이동재` - `박희원` - `이찬주` - `민채정`

---



### [2021.02.06]

__`Topic`__  **:** **Lecture 9: Generalization (이동재)**

__`Notes`__ : 

- [https://drive.google.com/file/d/1a55T8pqsaTU5RzoZOEmFMj01QcK1DFSk/view?usp=sharing](https://drive.google.com/file/d/1a55T8pqsaTU5RzoZOEmFMj01QcK1DFSk/view?usp=sharing) 

__`Links`__ : 

- [https://blog.naver.com/mykepzzang/220837959160](https://blog.naver.com/mykepzzang/220837959160) (베이즈 에러 도출 과정 확률과 표준편차 참고자료)
- [https://warm-uk.tistory.com/44](https://warm-uk.tistory.com/44) (BottleNeck 층을 활용한 Inception)
- [https://munjeongkang.github.io/ANN2/](https://munjeongkang.github.io/ANN2/) (model capacity)
- [https://light-tree.tistory.com/216](https://light-tree.tistory.com/216) (Weight decay , L1&L2 regularizer)
- [https://hyeonnii.tistory.com/353](https://hyeonnii.tistory.com/353) (Bayes Error)

__`Next`__ : 2021.02.20 9:00 PM KST

- [http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/) 
- __Lecture 11: Convolutional Networks (박희원)__ 

---



### [2021.02.20]

__`Topic`__ : __Lecture 11: Convolutional Networks (박희원)__ 

__`Notes`__ : 

- [https://drive.google.com/file/d/1G-xKWv8c00HsdIKE-GyhsE9mK_1GQJiq/view?usp=sharing](https://drive.google.com/file/d/1G-xKWv8c00HsdIKE-GyhsE9mK_1GQJiq/view?usp=sharing) 

__`Links`__ : 

- [https://untitledtblog.tistory.com/150](https://untitledtblog.tistory.com/150) 
- [https://underflow101.tistory.com/44](https://underflow101.tistory.com/44) 
- [https://underflow101.tistory.com/25](https://underflow101.tistory.com/25) 
- [https://excelsior-cjh.tistory.com/180](https://excelsior-cjh.tistory.com/180) 
- [https://dsbook.tistory.com/59](https://dsbook.tistory.com/59) 
- [https://nittaku.tistory.com/264](https://nittaku.tistory.com/264) 

__`Next`__ : 2021.02.27 9:00 PM KST

- [http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/) 
- **Lecture 12: Image Classification (이찬주)** 

```
1.두 데이터 셋 간단 소개 (MNIST , Caltech101)
2.Image NET 대회 간단 소개
3.Conv net 사이즈 구하는 방법 (표기 방법)
4.LeNET , AlexNET , GoogleNET , (LeNET , AlexNET 은 너무 깊게할 필요 없고 GoogleNET은 fully convolutional 이라는 의미가 뭔지)
```

---



### [2021.02.27]

__`Topic`__ : __Lecture 12: Image Classification (이찬주) , CNN misc part (이동재)__ 

__`Notes`__ : 

- [https://drive.google.com/file/d/1fOBJYnL4zsWlc9MFGb5MhZa_iOhknaz7/view?usp=sharing](https://drive.google.com/file/d/1fOBJYnL4zsWlc9MFGb5MhZa_iOhknaz7/view?usp=sharing) (이찬주)
- [https://drive.google.com/file/d/1EVjse-RIuFQ9s8GzWP_rPz8XtzXUYA05/view?usp=sharing](https://drive.google.com/file/d/1EVjse-RIuFQ9s8GzWP_rPz8XtzXUYA05/view?usp=sharing) (이동재)

__`Links`__ : 

- [https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec12.pdf](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/slides/lec12.pdf) (Lecture 12 slide) 
- [https://yjjo.tistory.com/8](https://yjjo.tistory.com/8) (start of CNN , concept of receptive field , how to calculate Conv. net size) 
- [https://bskyvision.com/421](https://bskyvision.com/421) (all about AlexNet : info-share , ReLU , local response normalization) 
- [https://bskyvision.com/422](https://bskyvision.com/422) (what is top-5 , top-1 error ?)

__`Next`__ : 2021.03.06 9:00 PM KST

- [https://m.blog.naver.com/laonple/220710707354](https://m.blog.naver.com/laonple/220710707354) (라온피플) 
- **Special Lecture : Inception , VGG Net , Res Net (이동재 , 민채정)** 

> __Google Net (이동재)__ 
>
> - 1X1 Convolution
> - Inception module (different kernel size , naive & advanced)
> - global average pooling
> - auxilary classifier

> __VGG Net , Res Net (민채정)__ 
>
> - important features of vgg net ( diff between 16 & 19 ?)
> - concept of skip connection from Res Net 
> - R-CNN , Fast R-CNN , Faster R-CNN 은 이번 파트 X

***



### [2021.03.06]

__`Topic`__ : __Lecture 12.3: GooLeNet (이동재)__ 

__`Notes`__ : 

- [https://drive.google.com/file/d/1_PMmX5JODykn1q1667IbfDbSi5moxNY-/view?usp=sharing](https://drive.google.com/file/d/1_PMmX5JODykn1q1667IbfDbSi5moxNY-/view?usp=sharing) (이동재)

__`Links`__ : 

- [https://blog.naver.com/laonple/220686328027](https://blog.naver.com/laonple/220686328027) (GooLeNet Lecture Note 1~5) 
- [https://bskyvision.com/539](https://bskyvision.com/539) (Inception V1 soft review) 
- [https://89douner.tistory.com/62](https://89douner.tistory.com/62) (concatenation in Inception module) 
- [https://jetsonaicar.tistory.com/16](https://jetsonaicar.tistory.com/16) (Global Average Pooling , explained)
- [https://lv99.tistory.com/21](https://lv99.tistory.com/21) (1X1 Conv. layer , explained)

__`Next`__ : 2021.03.13 9:00 PM KST

- **Special Lecture : VGG Net , Res Net (민채정)** 

- [https://blog.naver.com/laonple/220738560542](https://blog.naver.com/laonple/220738560542) (VGG Net [1] ~ VGG Net [2])
- [https://blog.naver.com/laonple/220761052425](https://blog.naver.com/laonple/220761052425) (Res Net [1] ~ Res Net [3])

> __Will Cover__ 
>
> _VGG Net_ : using only 3X3 kernel ? (what is factorizing colvolution filter ?)
>
> _VGG Net_ : how to deal with gradient vanish/exploding problem (pre-trained kernel initializing)
>
> _VGG Net_ : technique on how-to train/test dataset (scale jittering) <- 어려운 개념이니 간단하게만
>
> _Rest Net_ : what is residual learning? ( Shortcut-connection? Identity mapping?)
>
> _Rest Net_ : what features resnet team took from VGG? (common vs. diff)
>
> _Rest Net_ : BottleNeck Layer (only for models with layers>50)
>
> _Rest Net_ : other experiment with CIFAR dataset (going for 1000 layers)